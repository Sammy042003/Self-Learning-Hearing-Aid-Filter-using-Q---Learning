This research presents a novel adaptive filter selection framework for hearing aid systems using a lightweight, model-free reinforcement learning algorithm — Q-learning. Unlike traditional fixed-band filtering, the proposed system uses real-time audio features — Signal-to-Noise Ratio (SNR), Zero Crossing Rate (ZCR), and Spectral Centroid — to classify the acoustic environment into discrete states. The Q-learning agent dynamically selects from a predefined set of bandpass filter and gain actions, learning over time to maximize speech enhancement quality based on PESQ (Perceptual Evaluation of Speech Quality) and STOI (Short-Time Objective Intelligibility) metrics. Trained on the NOIZEUS dataset, which includes ten realworld noise types, the agent demonstrated robust adaptability, outperforming traditional filtering methods. This system is lightweight, interpretable, and suitable for real-time deployment in hearing aids, unlike black-box deep reinforcement learning models that require large computational resources. The proposed method bridges classical signal processing and intelligent adaptive control, offering a practical solution for hearing impaired users in dynamically noisy environments.

